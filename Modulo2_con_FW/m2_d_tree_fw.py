# -*- coding: utf-8 -*-
"""M2_d_tree_FW.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X1Hpuv5llX6TzEou8VtCqsp47ZTtvoy-

# Momento de Retroalimentación: Módulo 2 Uso de framework o biblioteca de aprendizaje máquina para la implementación de una solución. (Portafolio Implementación)

### Importar librerias
Se importan las bibliotecas necesarias como numpy, pandas y algunas funciones específicas de scikit-learn.
"""

# Importación de las bibliotecas necesarias
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import confusion_matrix, classification_report

"""### Leer los datos

Pide al usuario la ubicación del archivo CSV a cargar y luego lo lee utilizando la biblioteca pandas.
"""

# Carga del dataset
filename = input('''Ingresa la ruta donde se encuentra el archivo + /filename.csv
                 => ''')
df = pd.read_csv(filename, skiprows=1, header=None)

# Extracción de características y etiquetas
X = df.iloc[:, :-1].values
y = df.iloc[:, -1].values

# División del dataset en conjuntos de entrenamiento y temporal (test + validación)
X_train_temp, X_temp, y_train_temp, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)

# División del dataset en conjuntos de entrenamiento y temporal (test + validación)
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)

# División del conjunto temporal en conjuntos de prueba y validación
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)


# Lista para almacenar métricas
accuracies = []

# Entrenar el árbol incrementando su profundidad para ver cómo aprende
for depth in range(1, 11):  # profundidad máxima del modelo de
    # Inicializa y entrena el modelo
    clf = DecisionTreeClassifier(max_depth=depth)
    clf.fit(X_train, y_train)

    # Realiza predicciones y calcula la precisión
    y_pred = clf.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    accuracies.append(acc)

    # Imprimir métricas
    print(f"Profundidad: {depth}")
    print(f"Accuracy: {acc:.2f}")
    print(f"Precision: {precision_score(y_test, y_pred, average='weighted'):.2f}")
    print(f"Recall: {recall_score(y_test, y_pred, average='weighted'):.2f}")
    print(f"F1 Score: {f1_score(y_test, y_pred, average='weighted'):.2f}")

    # Matriz de confusión y reporte de clasificación
    print("Matriz de Confusión:")
    print(confusion_matrix(y_test, y_pred))
    print("\nReporte de Clasificación:")
    print(classification_report(y_test, y_pred))
    print("----------")

# Gráfica de precisión en función de la profundidad del modelo
plt.plot(range(1, 11), accuracies)
plt.title('Accuracy en función de la profundidad del árbol')
plt.xlabel('Profundidad')
plt.ylabel('Accuracy')
plt.xticks(range(1, 11))
plt.show()

"""### Visualización
Muestra una gráfica de precisión (accuracy) en función de la profundidad del árbol para visualizar cómo va aprendiendo el modelo.
"""

# Listas para almacenar métricas
train_errors, val_errors = [], []

# Evaluar la precisión a medida que aumenta el tamaño del conjunto de entrenamiento
for m in range(1, len(X_train)):
    clf = DecisionTreeClassifier(max_depth=10)
    clf.fit(X_train[:m], y_train[:m])

    y_train_predict = clf.predict(X_train[:m])
    y_val_predict = clf.predict(X_test)

    train_errors.append(accuracy_score(y_train[:m], y_train_predict))
    val_errors.append(accuracy_score(y_test, y_val_predict))

# Gráficar la curva de aprendizaje
plt.plot(np.sqrt(train_errors), "r-+", linewidth=2, label="train")
plt.plot(np.sqrt(val_errors), "b-", linewidth=3, label="val")
plt.title('Curva de Aprendizaje')
plt.xlabel('Tamaño del conjunto de entrenamiento')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""### Grado de bias

Se ha calculado el bias como la diferencia entre el accuracy ideal (1.0) y el accuracy promedio del conjunto de validación.

#### Grado de varianza

 Se calcula directamente como la varianza del accuracy en el conjunto de validación.
"""

# Cálculo del bias y varianza
bias = 1 - np.mean(val_errors)
variance = np.var(val_errors)

def categorizar_medida(medida):
    if medida < 0.1:
        return "Bajo"
    elif medida < 0.3:
        return "Medio"
    else:
        return "Alto"

bias_categoria = categorizar_medida(bias)
varianza_categoria = categorizar_medida(variance)

print(f"Grado de Bias (Sesgo): {bias:.4f} - {bias_categoria}")
print(f"Grado de Varianza: {variance:.4f} - {varianza_categoria}")

"""### Nivel de ajuste del modelo

Se determina por las características generales de bias y varianza.

1. Si ambos son altos, el modelo está underfitting y overfitting al mismo tiempo.
2. Si solo el bias es alto, es underfitting.
3. Si solo la varianza es alta, es overfitting.
4. Si ambos son bajos, el modelo está equilibrado.
"""

if bias_categoria == "Alto" and varianza_categoria == "Alto":
    print("El modelo presenta alto bias y alta varianza (underfitting y overfitting)")
elif bias_categoria == "Alto":
    print("El modelo presenta alto bias (underfitting)")
elif varianza_categoria == "Alto":
    print("El modelo presenta alta varianza (overfitting)")
else:
    print("El modelo tiene un buen equilibrio entre bias y varianza (fitt)")

"""### Gráficas de SESGO, VARIANZA Y NIVEL DE AJUSTE"""

# Gráficar Sesgo, Varianza y Nivel de Ajuste en subplots
fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(8, 12))

# Sesgo (Bias)
bias_array = [1 - acc for acc in train_errors]
ax[0].plot(range(1, len(train_errors) + 1), bias_array, label="Sesgo (1 - Train Accuracy)", color="blue")
ax[0].set_title("Sesgo en función del tamaño del conjunto de entrenamiento")
ax[0].set_xlabel("Tamaño del conjunto de entrenamiento")
ax[0].set_ylabel("Sesgo")
ax[0].legend()

# Varianza
variance_array = [val_errors[i] - train_errors[i] for i in range(len(val_errors))]
ax[1].plot(range(1, len(val_errors) + 1), variance_array, label="Varianza", color="red")
ax[1].set_title("Varianza en función del tamaño del conjunto de entrenamiento")
ax[1].set_xlabel("Tamaño del conjunto de entrenamiento")
ax[1].set_ylabel("Varianza")
ax[1].legend()

# Nivel de Ajuste (Overfitting/Underfitting)
ax[2].plot(range(1, len(train_errors) + 1), train_errors, label="Train Accuracy", color="blue")
ax[2].plot(range(1, len(val_errors) + 1), val_errors, label="Validation Accuracy", color="red")
ax[2].set_title("Nivel de Ajuste en función del tamaño del conjunto de entrenamiento")
ax[2].set_xlabel("Tamaño del conjunto de entrenamiento")
ax[2].set_ylabel("Accuracy")
ax[2].legend()

plt.tight_layout()
plt.show()